### Брейншторм по задачам

---

#### **1. EDA (Exploratory Data Analysis)**

**Цель**: Исследовать данные, чтобы понять их структуру, выявить закономерности и аномалии.

**Шаги**:
1. **Базовые статистики**:
   - Распределение длин текстов (промтов и ответов).
   - Количество уникальных языков и их распределение.
   - Средняя длина текста по языкам.
2. **Визуализация**:
   - Гистограммы длин текстов.
   - Графики распределения языков.
4. **Анализ побед моделей**:
   - В каком проценте случаев каждая модель побеждает.
   - Зависимость победы модели от длины текста, языка или других факторов.

---

#### **2. Токенизация иероглифов**

**Проблема**: Иероглифы (например, китайские, японские) требуют специальной обработки, так как стандартные токенизаторы (например, для английского) могут не справляться.

**Решение**:
1. Использовать специализированные токенизаторы:
   - Для китайского: **Jieba**.
   - Для японского: **MeCab** или **Sudachi**.
   - Для корейского: **KoNLPy**.
2. Использовать многоязычные токенизаторы:
   - **SentencePiece** (универсальный, поддерживает иероглифы).
   - **BERT Tokenizer** (многоязычный, но может разбивать иероглифы на subwords).
3. Проверить качество токенизации:
   - Убедиться, что иероглифы не разбиваются на бессмысленные части.
   - Сравнить результаты с ручной разметкой.

---

#### **3. Подходы к решению задачи**

**Цель**: Эффективно использовать текстовые данные (промты, ответы) для обучения или анализа. "Эффективно" = "протестировать гипотезы, потенциально позволяющие улучшить результат".

1. **Используем LLM по максимум**:
   >  А давайте всё сконкатенируем и закинем в LLM (Или любую другую трансформерную модель)?`
   - Собрать весь контекст (промт + ответы) в один большой текст.
   - Проблема: Ограниченное контекстное окно моделей. Требовательность к вычислительным ресурсам.
   - Решение: Использовать модели с поддержкой длинных контекстов (например, **Longformer** или **GPT-4** с увеличенным контекстом). Или смешанных идей, которые позволяют "расширять" контекст. (Какая-нибудь промежуточная агрегация или другие подходы). Или простой truncation по значимым частям каждого из текстовых блоков. 
  
2. **Эмбеддинги + Градиентный бустинг**:
   - Преобразовать тексты в эмбеддинги с помощью моделей (например, **BERT**, **Sentence-BERT**). Или 
   - Использовать эмбеддинги как фичи для CatBoost, LGBM или XGB.
   - Преимущество: Быстрее, чем LLM, и хорошо работает с табличными данными. Сможем хорошо агрегировать контекст. Ограниченное количество признаков для классификатора. (В отличие от TF-IDF =])

3. **Упор на наиболее частотные языки**
   > Игнорируем всё, кроме английского языка
   - Примеров на английском языке абсолютное большинство. Научимся хорошо классифицировать английский -- победим в сореве. 
   - Проблема: много инстансов не только на английском языке. (+ русский, китайский, вьетнамский, немецкий)
   - Решение: см. п. 4.

4. **Модели по языковым группам**:
   - Разделить данные по языковым группам. Выбрать лучшие для своей языковой группы. (Известный факт, что модели перформят лучше внутри своей языковой группы.) Возможная причина -- сходство языковой логики.
   - (До)Обучить отдельные модели для каждой языковой группы.
   - Преимущество: Меньше вычислительных ресурсов, лучше качество для каждого языка. Научимся решать хорошо -- будем молодцы.
   - Нормализация: Привести данные к единому формату (например, токенизация, очистка текста).
