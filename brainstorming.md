### Брейншторм по задачам

---

#### **1. EDA (Exploratory Data Analysis)**

**Цель**: Исследовать данные, чтобы понять их структуру, выявить закономерности и аномалии.

**Шаги**:
1. **Базовые статистики**:
   - Распределение длин текстов (промтов и ответов).
   - Количество уникальных языков и их распределение.
   - Средняя длина текста по языкам.
2. **Визуализация**:
   - Гистограммы длин текстов.
   - Графики распределения языков.
4. **Анализ побед моделей**:
   - В каком проценте случаев каждая модель побеждает.
   - Зависимость победы модели от длины текста, языка или других факторов.

---

#### **2. Токенизация иероглифов**

**Проблема**: Иероглифы (например, китайские, японские) требуют специальной обработки, так как стандартные токенизаторы (например, для английского) могут не справляться.

**Решение**:
1. Использовать специализированные токенизаторы:
   - Для китайского: **Jieba**.
   - Для японского: **MeCab** или **Sudachi**.
   - Для корейского: **KoNLPy**.
2. Использовать многоязычные токенизаторы:
   - **SentencePiece** (универсальный, поддерживает иероглифы).
   - **BERT Tokenizer** (многоязычный, но может разбивать иероглифы на subwords).
3. Проверить качество токенизации:
   - Убедиться, что иероглифы не разбиваются на бессмысленные части.
   - Сравнить результаты с ручной разметкой.

---

#### **3. Как использовать тексты**

**Цель**: Эффективно использовать текстовые данные (промты, ответы) для обучения или анализа.

**Варианты**:
1. **Использование LLM (Large Language Model)**:
   - Собрать весь контекст (промт + ответы) в один большой текст.
   - Проблема: Вычислительная сложность, особенно для длинных текстов.
   - Решение: Использовать модели с поддержкой длинных контекстов (например, **Longformer** или **GPT-4** с увеличенным контекстом).
2. **Эмбеддинги + CatBoost**:
   - Преобразовать тексты в эмбеддинги с помощью моделей (например, **BERT**, **Sentence-BERT**).
   - Использовать эмбеддинги как фичи для CatBoost.
   - Преимущество: Быстрее, чем LLM, и хорошо работает с табличными данными.
3. **Модели по языковым группам**:
   - Разделить данные по языкам.
   - Обучить отдельные модели для каждой языковой группы.
   - Преимущество: Меньше вычислительных ресурсов, лучше качество для каждого языка.
   - Нормализация: Привести данные к единому формату (например, токенизация, очистка текста).

---
