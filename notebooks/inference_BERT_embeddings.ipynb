{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T07:26:11.654611Z",
     "iopub.status.busy": "2025-01-24T07:26:11.654119Z",
     "iopub.status.idle": "2025-01-24T07:26:11.669509Z",
     "shell.execute_reply": "2025-01-24T07:26:11.668652Z",
     "shell.execute_reply.started": "2025-01-24T07:26:11.654568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T07:26:13.803341Z",
     "iopub.status.busy": "2025-01-24T07:26:13.802897Z",
     "iopub.status.idle": "2025-01-24T07:26:15.667508Z",
     "shell.execute_reply": "2025-01-24T07:26:15.666462Z",
     "shell.execute_reply.started": "2025-01-24T07:26:13.803301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path=\"../data/\"\n",
    "train = pd.read_parquet(path+\"train.parquet\")\n",
    "\n",
    "train, test = train_test_split(train, test_size=0.2, random_state=1)\n",
    "eval, test = train_test_split(test, test_size=0.5, random_state=1)\n",
    "\n",
    "sub = pd.read_csv(path+\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T07:26:15.669477Z",
     "iopub.status.busy": "2025-01-24T07:26:15.669076Z",
     "iopub.status.idle": "2025-01-24T07:26:15.682536Z",
     "shell.execute_reply": "2025-01-24T07:26:15.681524Z",
     "shell.execute_reply.started": "2025-01-24T07:26:15.669434Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18632</th>\n",
       "      <td>623e2aa50f13aece7fb1c60cf555d8c7d290692f679655...</td>\n",
       "      <td>Phân tích kết quả sản xuất (KQSX) về chất lượn...</td>\n",
       "      <td>Để phân tích kết quả sản xuất (KQSX) về chất l...</td>\n",
       "      <td>Phân tích kết quả sản xuất (KQSX) về chất lượn...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>yi-lightning</td>\n",
       "      <td>internlm2_5-20b-chat</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34859</th>\n",
       "      <td>b87079eef09873032f653314cfddd05e7bf77e4e6d0449...</td>\n",
       "      <td>Write a sample of Old Uyghur</td>\n",
       "      <td>Providing a truly representative sample of Old...</td>\n",
       "      <td>I apologize, but I cannot generate an authenti...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gemini-1.5-flash-002</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43764</th>\n",
       "      <td>e721d5bfbff8e05e2ffa92e0137a570c1b1b7fa3f7bbf7...</td>\n",
       "      <td>Write a poem in the style of Ada Limon.</td>\n",
       "      <td>**Under the Bruise of the World**\\n\\nIn the ga...</td>\n",
       "      <td>Here's a poem inspired by the style of Ada Lim...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>reka-core-20240722</td>\n",
       "      <td>grok-2-2024-08-13</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22541</th>\n",
       "      <td>77350a005d0195fa951b9fb073b018f2f6424a58ee58b5...</td>\n",
       "      <td>describe the way \"golden slumbers\" by the beat...</td>\n",
       "      <td>Describing the way \"Golden Slumbers\" by The Be...</td>\n",
       "      <td>For a deaf person who has never heard \"Golden ...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>jamba-1.5-large</td>\n",
       "      <td>deepseek-coder-v2-0724</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37157</th>\n",
       "      <td>c43d63ee99a8b776065f410e4a1e0791a4449230cfd648...</td>\n",
       "      <td>かけっこをしています。あなたは3位の人を抜かし ました。今あなたは何位？</td>\n",
       "      <td>3位です。あなたは3位の人を抜かしたので、今あなたは3位になります。</td>\n",
       "      <td>はい、かけっこの状況について考えてみましょう。\\n\\n問題文から、以下のことがわかります。\\...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>claude-3-opus-20240229</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      id  \\\n",
       "18632  623e2aa50f13aece7fb1c60cf555d8c7d290692f679655...   \n",
       "34859  b87079eef09873032f653314cfddd05e7bf77e4e6d0449...   \n",
       "43764  e721d5bfbff8e05e2ffa92e0137a570c1b1b7fa3f7bbf7...   \n",
       "22541  77350a005d0195fa951b9fb073b018f2f6424a58ee58b5...   \n",
       "37157  c43d63ee99a8b776065f410e4a1e0791a4449230cfd648...   \n",
       "\n",
       "                                                  prompt  \\\n",
       "18632  Phân tích kết quả sản xuất (KQSX) về chất lượn...   \n",
       "34859                       Write a sample of Old Uyghur   \n",
       "43764            Write a poem in the style of Ada Limon.   \n",
       "22541  describe the way \"golden slumbers\" by the beat...   \n",
       "37157               かけっこをしています。あなたは3位の人を抜かし ました。今あなたは何位？   \n",
       "\n",
       "                                              response_a  \\\n",
       "18632  Để phân tích kết quả sản xuất (KQSX) về chất l...   \n",
       "34859  Providing a truly representative sample of Old...   \n",
       "43764  **Under the Bruise of the World**\\n\\nIn the ga...   \n",
       "22541  Describing the way \"Golden Slumbers\" by The Be...   \n",
       "37157                 3位です。あなたは3位の人を抜かしたので、今あなたは3位になります。   \n",
       "\n",
       "                                              response_b   winner  \\\n",
       "18632  Phân tích kết quả sản xuất (KQSX) về chất lượn...  model_a   \n",
       "34859  I apologize, but I cannot generate an authenti...  model_a   \n",
       "43764  Here's a poem inspired by the style of Ada Lim...  model_b   \n",
       "22541  For a deaf person who has never heard \"Golden ...  model_a   \n",
       "37157  はい、かけっこの状況について考えてみましょう。\\n\\n問題文から、以下のことがわかります。\\...  model_a   \n",
       "\n",
       "                    model_a                     model_b    language  \n",
       "18632          yi-lightning        internlm2_5-20b-chat  Vietnamese  \n",
       "34859  gemini-1.5-flash-002  claude-3-5-sonnet-20240620     English  \n",
       "43764    reka-core-20240722           grok-2-2024-08-13     English  \n",
       "22541       jamba-1.5-large      deepseek-coder-v2-0724     English  \n",
       "37157            o1-preview      claude-3-opus-20240229    Japanese  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T07:26:15.684463Z",
     "iopub.status.busy": "2025-01-24T07:26:15.684105Z",
     "iopub.status.idle": "2025-01-24T07:26:15.706847Z",
     "shell.execute_reply": "2025-01-24T07:26:15.705542Z",
     "shell.execute_reply.started": "2025-01-24T07:26:15.684426Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7419</th>\n",
       "      <td>271ba404fc25609b8ceb6f37c3375f278c42e2479514ea...</td>\n",
       "      <td>Улучшь сообщение: Добрый вечер! \\nЭто Алексей ...</td>\n",
       "      <td>Добрый вечер!\\n\\nМеня зовут Алексей Королев, я...</td>\n",
       "      <td>Добрый вечер!\\n\\nЯ, Алексей Королев, заместите...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>qwen-max-0919</td>\n",
       "      <td>c4ai-aya-expanse-32b</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39112</th>\n",
       "      <td>ce7be103ac8f040accc9b8ec421a6cbf78189c503671df...</td>\n",
       "      <td>i have a notification icon in my webpage. when...</td>\n",
       "      <td>Certainly! To implement a notification system ...</td>\n",
       "      <td>Here's a simple example of how you can achieve...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>qwen2.5-72b-instruct</td>\n",
       "      <td>llama-3.1-405b-instruct-bf16</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33946</th>\n",
       "      <td>b39725e0a7992eb0c5f34542f960727a4fd306741568fb...</td>\n",
       "      <td>I have a raspberry pi that is booted from USB....</td>\n",
       "      <td>**Hotplugging the SD Card on a Raspberry Pi**\\...</td>\n",
       "      <td>No, you generally **cannot** safely hotplug th...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>llama-3.1-8b-instruct</td>\n",
       "      <td>gemini-1.5-pro-exp-0827</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35392</th>\n",
       "      <td>bb33f84e8e85da9e41fa271f6210fdffba0400d725a04c...</td>\n",
       "      <td>Imagine video on YouTube \"Asking people what i...</td>\n",
       "      <td>Here are some imagined YouTube comments relati...</td>\n",
       "      <td>Here are some comments that might appear under...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gemini-1.5-flash-002</td>\n",
       "      <td>gemma-2-27b-it</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10254</th>\n",
       "      <td>364c8146a6a236fbdfa34492dba10405a0e448d3062316...</td>\n",
       "      <td>generate a wine description</td>\n",
       "      <td>Please provide some context!  To generate a co...</td>\n",
       "      <td>**Wine Description:**\\n\\n**Name:** Sunset on t...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>gemini-1.5-flash-8b-001</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      id  \\\n",
       "7419   271ba404fc25609b8ceb6f37c3375f278c42e2479514ea...   \n",
       "39112  ce7be103ac8f040accc9b8ec421a6cbf78189c503671df...   \n",
       "33946  b39725e0a7992eb0c5f34542f960727a4fd306741568fb...   \n",
       "35392  bb33f84e8e85da9e41fa271f6210fdffba0400d725a04c...   \n",
       "10254  364c8146a6a236fbdfa34492dba10405a0e448d3062316...   \n",
       "\n",
       "                                                  prompt  \\\n",
       "7419   Улучшь сообщение: Добрый вечер! \\nЭто Алексей ...   \n",
       "39112  i have a notification icon in my webpage. when...   \n",
       "33946  I have a raspberry pi that is booted from USB....   \n",
       "35392  Imagine video on YouTube \"Asking people what i...   \n",
       "10254                        generate a wine description   \n",
       "\n",
       "                                              response_a  \\\n",
       "7419   Добрый вечер!\\n\\nМеня зовут Алексей Королев, я...   \n",
       "39112  Certainly! To implement a notification system ...   \n",
       "33946  **Hotplugging the SD Card on a Raspberry Pi**\\...   \n",
       "35392  Here are some imagined YouTube comments relati...   \n",
       "10254  Please provide some context!  To generate a co...   \n",
       "\n",
       "                                              response_b   winner  \\\n",
       "7419   Добрый вечер!\\n\\nЯ, Алексей Королев, заместите...  model_b   \n",
       "39112  Here's a simple example of how you can achieve...  model_b   \n",
       "33946  No, you generally **cannot** safely hotplug th...  model_a   \n",
       "35392  Here are some comments that might appear under...  model_a   \n",
       "10254  **Wine Description:**\\n\\n**Name:** Sunset on t...  model_b   \n",
       "\n",
       "                       model_a                       model_b language  \n",
       "7419             qwen-max-0919          c4ai-aya-expanse-32b  Russian  \n",
       "39112     qwen2.5-72b-instruct  llama-3.1-405b-instruct-bf16  English  \n",
       "33946    llama-3.1-8b-instruct       gemini-1.5-pro-exp-0827  English  \n",
       "35392     gemini-1.5-flash-002                gemma-2-27b-it  English  \n",
       "10254  gemini-1.5-flash-8b-001        llama-3.1-70b-instruct  English  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "\n",
    "model.cls = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_token_length(text: str, tokenizer) -> int:\n",
    "    tok_out = tokenizer(text)\n",
    "    return len(tok_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48439 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (694 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 48439/48439 [00:14<00:00, 3366.65it/s]\n",
      "100%|██████████| 48439/48439 [00:30<00:00, 1610.67it/s]\n",
      "100%|██████████| 48439/48439 [00:30<00:00, 1613.45it/s]\n"
     ]
    }
   ],
   "source": [
    "train_prompt_length = train['prompt'].progress_apply(get_text_token_length, args=(tokenizer,))\n",
    "train_response_a_length = train['response_a'].progress_apply(get_text_token_length, args=(tokenizer,))\n",
    "train_response_b_length = train['response_b'].progress_apply(get_text_token_length, args=(tokenizer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_prompt_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_prompt_length\u001b[49m.max(), train_prompt_length.min())\n\u001b[32m      2\u001b[39m plt.hist(train_prompt_length, bins=\u001b[32m200\u001b[39m);\n",
      "\u001b[31mNameError\u001b[39m: name 'train_prompt_length' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_prompt_length.max(), train_prompt_length.min())\n",
    "plt.hist(train_prompt_length, bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, device):\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        encoded = (\n",
    "            tokenizer(row[\"prompt\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True),\n",
    "            tokenizer(row[\"response_a\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True),\n",
    "            tokenizer(row[\"response_b\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True),\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            {key: tensor.squeeze(0).to(self.device) for key, tensor in encoded[0].items()},\n",
    "            {key: tensor.squeeze(0).to(self.device) for key, tensor in encoded[1].items()},\n",
    "            {key: tensor.squeeze(0).to(self.device) for key, tensor in encoded[2].items()},\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "\n",
    "model.cls = nn.Identity()\n",
    "model = model.to(device)\n",
    "\n",
    "batch_size = 32\n",
    "train_data_loader = DataLoader(QADataset(train, device), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "eval_data_loader = DataLoader(QADataset(eval, device), batch_size=batch_size, drop_last=True)\n",
    "test_data_loader = DataLoader(QADataset(test, device), batch_size=batch_size, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_dataloader_embeddings(dataloader: DataLoader, model):\n",
    "    embeddings = {\n",
    "        \"prompt\": [], \n",
    "        \"response_a\": [], \n",
    "        \"response_b\": [], \n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        prompts = batch[0]\n",
    "        responses_a = batch[1]\n",
    "        responses_b = batch[2]\n",
    "\n",
    "        prompts_embs = model(**prompts)\n",
    "        response_a_embs = model(**responses_a)\n",
    "        response_b_embs = model(**responses_b)\n",
    "\n",
    "        # embeddings[\"prompt\"].append(prompts_embs.logits.mean(1).detach().cpu().numpy())\n",
    "        # embeddings[\"response_a\"].append(response_a_embs.logits.mean(1).detach().cpu().numpy())\n",
    "        # embeddings[\"response_b\"].append(response_b_embs.logits.mean(1).detach().cpu().numpy())\n",
    "\n",
    "        embeddings[\"prompt\"].append(prompts_embs.logits[:, 0, :].detach().cpu().numpy())\n",
    "        embeddings[\"response_a\"].append(response_a_embs.logits[:, 0, :].detach().cpu().numpy())\n",
    "        embeddings[\"response_b\"].append(response_b_embs.logits[:, 0, :].detach().cpu().numpy())\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1210 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1210/1210 [07:00<00:00,  2.87it/s]\n",
      "100%|██████████| 151/151 [00:52<00:00,  2.87it/s]\n",
      "100%|██████████| 151/151 [00:52<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_dataloader_embeddings(train_data_loader, model)\n",
    "eval_embeddings = get_dataloader_embeddings(eval_data_loader, model)\n",
    "test_embeddings = get_dataloader_embeddings(test_data_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4843/4843 [07:14<00:00, 11.15it/s]\n",
      "100%|██████████| 605/605 [00:54<00:00, 11.00it/s]\n",
      "100%|██████████| 605/605 [00:55<00:00, 10.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Эмбеддинг документа -- усреднение эмбеддингов токено\n",
    "train_embeddings = get_dataloader_embeddings(train_data_loader, model)\n",
    "eval_embeddings = get_dataloader_embeddings(eval_data_loader, model)\n",
    "test_embeddings = get_dataloader_embeddings(test_data_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_dataloader_embeddings(embeddings: dict[str, list[np.ndarray]]):\n",
    "    prompts_embs = np.concatenate(embeddings['prompt'])\n",
    "    response_a_embs = np.concatenate(embeddings['response_a'])\n",
    "    response_b_embs = np.concatenate(embeddings['response_b'])\n",
    "\n",
    "    all_embs = np.concatenate([prompts_embs, response_a_embs, response_b_embs], axis=1)\n",
    "    return all_embs\n",
    "\n",
    "train_features = aggregate_dataloader_embeddings(train_embeddings)\n",
    "eval_features = aggregate_dataloader_embeddings(eval_embeddings)\n",
    "test_features = aggregate_dataloader_embeddings(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38720 38720\n",
      "4832 4832\n",
      "4832 4832\n"
     ]
    }
   ],
   "source": [
    "X_train = train_features\n",
    "y_train = train['winner'].iloc[:len(X_train)]\n",
    "\n",
    "X_eval = eval_features\n",
    "y_eval = eval['winner'].iloc[:len(X_eval)]\n",
    "\n",
    "X_test = test_features\n",
    "y_test = test['winner'].iloc[:len(X_test)]\n",
    "\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_eval), len(y_eval))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.078186\n",
      "0:\tlearn: 0.5128874\ttest: 0.5060017\tbest: 0.5060017 (0)\ttotal: 116ms\tremaining: 1m 55s\n",
      "100:\tlearn: 0.6655217\ttest: 0.4935844\tbest: 0.5132450 (1)\ttotal: 5.11s\tremaining: 45.5s\n",
      "200:\tlearn: 0.7464360\ttest: 0.5057947\tbest: 0.5132450 (1)\ttotal: 9.89s\tremaining: 39.3s\n",
      "300:\tlearn: 0.8065341\ttest: 0.4877897\tbest: 0.5132450 (1)\ttotal: 14.9s\tremaining: 34.5s\n",
      "400:\tlearn: 0.8525052\ttest: 0.4952401\tbest: 0.5132450 (1)\ttotal: 20.7s\tremaining: 30.9s\n",
      "500:\tlearn: 0.8854597\ttest: 0.5057947\tbest: 0.5132450 (1)\ttotal: 26.2s\tremaining: 26.1s\n",
      "600:\tlearn: 0.9077738\ttest: 0.5051738\tbest: 0.5132450 (1)\ttotal: 31.7s\tremaining: 21.1s\n",
      "700:\tlearn: 0.9251808\ttest: 0.5097268\tbest: 0.5132450 (1)\ttotal: 37.2s\tremaining: 15.9s\n",
      "800:\tlearn: 0.9392045\ttest: 0.4946192\tbest: 0.5132450 (1)\ttotal: 42.6s\tremaining: 10.6s\n",
      "900:\tlearn: 0.9518595\ttest: 0.5045530\tbest: 0.5132450 (1)\ttotal: 48.3s\tremaining: 5.31s\n",
      "999:\tlearn: 0.9614928\ttest: 0.5060017\tbest: 0.5132450 (1)\ttotal: 53.7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5132450331\n",
      "bestIteration = 1\n",
      "\n",
      "Shrink model to first 2 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f934ff98d90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbm = CatBoostClassifier(random_state=42, eval_metric=\"Accuracy\", verbose=100)\n",
    "cbm.fit(X_train, y_train, eval_set=(X_eval, y_eval))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10131489,
     "sourceId": 86946,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
