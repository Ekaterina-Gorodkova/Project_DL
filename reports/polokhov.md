# Отчет о проделанной работе

## Часть 1

* Brainstorm подходов к решению задач, в частности:
  * TF-IDF/BOF + SVM/LogReg
  * TextCNN
  * BeRT-like training
  * GPT-like training
  * LLM Fine Tuning
* Проведено исследование существующих решений контеста. Самые преобладающие - LoRA или QLoRA файн тюнинг для LLM, таких как Gemma-2 или QWEN. Были произведены попытки файн тюнинга своих решений при помощи подобных подходов, но даже при достаточно маленьких архитектурах обучение занимает десятки, иногда сотни часов (на T4 или P100), а таких ресурсов у нас нет.
* Дальше буду пытаться Fine-tuning болей простых моделей или что-то из списка выше или координально другие подходы.
